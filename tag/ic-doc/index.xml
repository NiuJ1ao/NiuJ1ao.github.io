<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IC DoC | Yuchen Niu</title>
    <link>https://example.com/tag/ic-doc/</link>
      <atom:link href="https://example.com/tag/ic-doc/index.xml" rel="self" type="application/rss+xml" />
    <description>IC DoC</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 31 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hue3a655598c6865a5273e9cfb01f02a1f_2400_512x512_fill_lanczos_center_3.png</url>
      <title>IC DoC</title>
      <link>https://example.com/tag/ic-doc/</link>
    </image>
    
    <item>
      <title>Data Augmentation in Infinitely Wide Neural Networks.</title>
      <link>https://example.com/project/master_thesis/</link>
      <pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/master_thesis/</guid>
      <description>&lt;p&gt;Supervised by &lt;a href=&#34;https://mvdw.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr Mark van der Wilk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Infinitely wide neural networks have shown equivalence to Gaussian processes (GPs).This equivalence enables exact Bayesian inference for the posterior and predictivedistribution without ever instantiating a neural network, but by evaluating the corresponding GP. A major advantage of GPs over neural networks is that the hyperparameters can be optimised by backpropagation to maximise the marginal likelihood. However, the evaluation of the marginal likelihood is computationally intractable in large scale machine learning, especially when the training data is enlarged for better generalisation by generating a broader set of augmentations. In this project, we address the intractability by sparse Gaussian process regression and choose the bestmodel by maximising a variational lower bound of the true log marginal likelihood. We find that the sparse approximation estimates the true posterior on a regressiontask with few inducing points. However, it shows the difficulty of approximation on classification tasks. A comparison of the efficiency of different methods for selecting inducing points is also conducted. Furthermore, we study the invariances in the data and incorporate them into the model by summing over orbits. Empirical results show that the invariances can be learned in a supervised manner and demonstrate different characteristics from the infinitely wide neural networks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calibrating Uncertainty Estimates for Machine Learning Models</title>
      <link>https://example.com/project/iso/</link>
      <pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/iso/</guid>
      <description>&lt;p&gt;Supervised by &lt;a href=&#34;http://yingzhenli.net/home/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr Yingzhen Li&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recent advances in machine learning have dramatically improved the ability of neural
networks to solve real-world problems. Machine learning methods are also more frequently
deployed in critical scenarios. This requires that machine learning models need to be both
accurate and reliable. So, calibrating a model to create a rigorous confidence set or estimates
that represent the actual correctness likelihood is essential. In this report, we review the
theoretical and practical concepts of uncertainty estimation and calibration. The history of
popular calibration models and their application are also covered, which will help understand
concepts more intuitively. We conduct experiments on the uncertainty estimation of neural
networks before and after applying various post-processing calibration methods. Empirical
results show that modern neural networks are often overconfident and poorly calibrated.
Temperature scaling is straightforward and surprisingly efficient at calibrating confidence for
classification models. Conformal prediction is a non-parametric distribution-free calibration
method for regression models. However, we notice that distribution shifts and biases can
easily influence the confidence interval for predictions from conformal inference. Ensembling
can be an implicit calibration method that can mitigate the overconfidence of neural networks
and provide stable predictions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prompt Tuning for Condescending Detection</title>
      <link>https://example.com/project/icnlp/</link>
      <pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/icnlp/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
