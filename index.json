[{"authors":null,"categories":null,"content":"I am a master student at Imperial College London. Before that, I obtained my bachelor degree from School of Informatics at the University of Edinburgh.\nDownload my resumé.\n","date":1661904000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1661904000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a master student at Imperial College London. Before that, I obtained my bachelor degree from School of Informatics at the University of Edinburgh.\nDownload my resumé.","tags":null,"title":"Yuchen Niu","type":"authors"},{"authors":["Yuchen Niu"],"categories":null,"content":"Supervised by Dr Mark van der Wilk.\nInfinitely wide neural networks have shown equivalence to Gaussian processes (GPs).This equivalence enables exact Bayesian inference for the posterior and predictivedistribution without ever instantiating a neural network, but by evaluating the corresponding GP. A major advantage of GPs over neural networks is that the hyperparameters can be optimised by backpropagation to maximise the marginal likelihood. However, the evaluation of the marginal likelihood is computationally intractable in large scale machine learning, especially when the training data is enlarged for better generalisation by generating a broader set of augmentations. In this project, we address the intractability by sparse Gaussian process regression and choose the bestmodel by maximising a variational lower bound of the true log marginal likelihood. We find that the sparse approximation estimates the true posterior on a regressiontask with few inducing points. However, it shows the difficulty of approximation on classification tasks. A comparison of the efficiency of different methods for selecting inducing points is also conducted. Furthermore, we study the invariances in the data and incorporate them into the model by summing over orbits. Empirical results show that the invariances can be learned in a supervised manner and demonstrate different characteristics from the infinitely wide neural networks.\n","date":1661904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661904000,"objectID":"57dfd8517a0a70434e1453262850c81d","permalink":"https://example.com/project/master_thesis/","publishdate":"2022-08-31T00:00:00Z","relpermalink":"/project/master_thesis/","section":"project","summary":"Supervised by Dr Mark van der Wilk.\nInfinitely wide neural networks have shown equivalence to Gaussian processes (GPs).This equivalence enables exact Bayesian inference for the posterior and predictivedistribution without ever instantiating a neural network, but by evaluating the corresponding GP.","tags":["IC DoC"],"title":"Data Augmentation in Infinitely Wide Neural Networks.","type":"project"},{"authors":["Yuchen Niu"],"categories":[],"content":"Supervised by Dr Yingzhen Li.\nRecent advances in machine learning have dramatically improved the ability of neural networks to solve real-world problems. Machine learning methods are also more frequently deployed in critical scenarios. This requires that machine learning models need to be both accurate and reliable. So, calibrating a model to create a rigorous confidence set or estimates that represent the actual correctness likelihood is essential. In this report, we review the theoretical and practical concepts of uncertainty estimation and calibration. The history of popular calibration models and their application are also covered, which will help understand concepts more intuitively. We conduct experiments on the uncertainty estimation of neural networks before and after applying various post-processing calibration methods. Empirical results show that modern neural networks are often overconfident and poorly calibrated. Temperature scaling is straightforward and surprisingly efficient at calibrating confidence for classification models. Conformal prediction is a non-parametric distribution-free calibration method for regression models. However, we notice that distribution shifts and biases can easily influence the confidence interval for predictions from conformal inference. Ensembling can be an implicit calibration method that can mitigate the overconfidence of neural networks and provide stable predictions.\n","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651795200,"objectID":"caa25a149fb569acd28a4ea4e59734d1","permalink":"https://example.com/project/iso/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/project/iso/","section":"project","summary":"Supervised by Dr Yingzhen Li.\nRecent advances in machine learning have dramatically improved the ability of neural networks to solve real-world problems. Machine learning methods are also more frequently deployed in critical scenarios.","tags":["IC DoC","Coursework"],"title":"Calibrating Uncertainty Estimates for Machine Learning Models","type":"project"},{"authors":["Yuchen Niu"],"categories":[],"content":"","date":1646352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646352000,"objectID":"694ee320ccdcdbeb39f9efdccc67c9a9","permalink":"https://example.com/project/icnlp/","publishdate":"2022-03-04T00:00:00Z","relpermalink":"/project/icnlp/","section":"project","summary":"","tags":["IC DoC","Coursework"],"title":"Prompt Tuning for Condescending Detection","type":"project"},{"authors":["Cunxiang Wang","Boyuan Zheng","Yuchen Niu","Yue Zhang"],"categories":null,"content":" ","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"ab8cf6b15e3a5a0b12693103951be145","permalink":"https://example.com/publication/plm_arithmetic_logic/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/publication/plm_arithmetic_logic/","section":"publication","summary":"To quantitatively and intuitively explore the generalization ability of pre-trained language models (PLMs), we have designed several tasks of arithmetic and logical reasoning. We both analyse how well PLMs generalize when the test data is in the same distribution as the train data and when it is different, for the latter analysis, we have also designed a cross-distribution test set other than the in-distribution test set. We conduct experiments on one of the most advanced and publicly released generative PLM - BART. Our research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.","tags":[],"title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning","type":"publication"},{"authors":["Yuchen Niu"],"categories":null,"content":"Supervised by Dr Boris Grot and Dr Dmitrii Ustiugov.\nServerless computing is a growing cloud framework that removes the barriers of resource provision and scaling from the cloud users. Function-as-a-Service (FaaS) is a way to deploy an application, as a set of functions, to serverless backends in the cloud. The functions are usually short-lived and have unpredictable invocation patterns. A typical cloud environment has multiple tenants coreside on a server. The combination of the multitenancy and functions’ characterization leads to that a serverless platform has to address the potential overheads of burstable invocations. This project studies the architectural implications of serverless computing under a high multitenancy. We explicitly measure the throughput and hardware performance counters of a server while the number of tenants varies. We find that the server suffers an approximately 40% of throughput loss when the number of tenants exceeds the number of physical cores. The characteristics of hardware performance counters show that the contention of data demands on the memory subsystem is rapidly aggravated as the number of tenants increases. When two tenants coreside on the same physical core, the contention on the instruction fetch may overtake the stalls in the memory subsystem. We also conform that the interleaves short functions from multiple tenants are harmful to the localitypreserving microarchitectural structures. We open-source an automated profiling tool in the vHive, which we developed for this project.\n","date":1618185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618185600,"objectID":"a203daa28124b31323ea595b41a2c343","permalink":"https://example.com/project/ug_thesis/","publishdate":"2021-04-12T00:00:00Z","relpermalink":"/project/ug_thesis/","section":"project","summary":"Supervised by Dr Boris Grot and Dr Dmitrii Ustiugov.\nServerless computing is a growing cloud framework that removes the barriers of resource provision and scaling from the cloud users. Function-as-a-Service (FaaS) is a way to deploy an application, as a set of functions, to serverless backends in the cloud.","tags":["UoE Informatics","Serverless Computing","Performance Analysis"],"title":"Performance Characterization of Serverless Computing","type":"project"},{"authors":["Yuchen Niu","Zhicheng Guo","Yiran Xu"],"categories":[],"content":"In this project, we studied the effectiveness of length ratio conditioning on paraphrase genera- tion. We constructed two datasets. One used a pretrained NMT model to construct a new paraphrasing dataset from WMT 2019 Europarl German-English dataset, and the other was ex- tracted from MSCOCO and QUORA question pairs dataset. We proposed tagging and fine- tuning methods on length ratio for condition- ing paraphrase generation. Finally we evaluate length-ratio conditioning performance of models from semantic similarity and length change. The major finding is that tagging had little to no effect on conditioning tasks while having a better para- phrase performance. In comparison, however, fine-tuning is more effective in conditioning but showed poor performance in paraphrasing.\n","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"7750d6696b05d993c82d4e2a10bf968c","permalink":"https://example.com/project/mlp/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/project/mlp/","section":"project","summary":"In this project, we studied the effectiveness of length ratio conditioning on paraphrase genera- tion. We constructed two datasets. One used a pretrained NMT model to construct a new paraphrasing dataset from WMT 2019 Europarl German-English dataset, and the other was ex- tracted from MSCOCO and QUORA question pairs dataset.","tags":["UoE Informatics","Coursework"],"title":"Machine Learning Practical","type":"project"},{"authors":[],"categories":[],"content":"","date":1586649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586649600,"objectID":"b861fc85f0c2b27859e62c3aa2c61a6b","permalink":"https://example.com/project/sdp/","publishdate":"2020-04-12T00:00:00Z","relpermalink":"/project/sdp/","section":"project","summary":"","tags":["UoE Informatics","Coursework"],"title":"System Design Project","type":"project"},{"authors":["Yuchen Niu"],"categories":[],"content":"","date":1555027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555027200,"objectID":"58c47694e6d1df02ad160fa29c16a166","permalink":"https://example.com/project/ilp/","publishdate":"2019-04-12T00:00:00Z","relpermalink":"/project/ilp/","section":"project","summary":"","tags":["UoE Informatics","Coursework"],"title":"Informatics Large Practical","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://example.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]